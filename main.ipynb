{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveNet architecture - a breif introduction\n",
    "Wavenet is an autoregressive model that use local and global embeddings to generate audio from transcripts. The local embeddings are used to convey local transcript information to the model i.e. what needs to be said and when. The global embeddings are transmitted to the entire model and can be used for e.g. speaker embeddings (generate sound in a specific voice). The model was originally praised for two things (1) producing audio that sounded very natural (2) for being parallelizable during training unilike RNNs.\n",
    "<br>\n",
    "<br>\n",
    "I'm not gonna explain WaveNet in details, because it would take way to much time do. I have however made a simple figure which can be seen below. This don't tell the whole story, but should hopefully get the main point(s) across. \n",
    "<br>\n",
    "<br>\n",
    "<p align=\"center\">\n",
    "  <img src=\"./readme_res/complete_arcitecture.png\" alt=\"Drawing\"/ width=1000>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Zohar Jackson's [free spoken digit dataset dataset](https://github.com/Jakobovski/free-spoken-digit-dataset) was used for training and evaluation\n",
    "* The model architecture was directly inspired by Google's original Wavenet paper [A Generative Model for Raw Audio](https://export.arxiv.org/abs/1609.03499)\n",
    "* Text to phoneme embeddings was done with Kyubyong Park's [g2p_en](https://github.com/Kyubyong/g2p)\n",
    "* r9y9's [WaveNet implementation](https://github.com/r9y9/wavenet_vocoder) was used for inspiration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.) Imports and config\n",
    "The config file contains all the hyperparameters (learning rate, epochs, etc.), control variables if debug mode, paths, etc.) and alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install g2p-en\n",
    "from g2p_en import G2p\n",
    "\n",
    "# Mine\n",
    "import helpers as H\n",
    "\n",
    "# Model stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "\n",
    "# All around\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython import display\n",
    "import IPython\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from IPython.core.display import display\n",
    "import seaborn as sns; sns.set_style(\"darkgrid\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Control\n",
    "    mode = \"train\"\n",
    "    debug = False\n",
    "    use_wandb = True\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    wandb_watch_activated = False\n",
    "    epochs_trained = 0\n",
    "    \n",
    "    # Dataset downloaded from: \n",
    "    # https://github.com/Jakobovski/free-spoken-digit-dataset\n",
    "    path_main = \"./data/\"\n",
    "    assert os.path.exists(path_main), \"path_main - Bad path\"\n",
    "\n",
    "    # General hypers\n",
    "    batch_size = 20\n",
    "    epochs = 100\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_hyper = dict(lr = 1e-3)\n",
    "    optimizer = torch.optim.Adam   \n",
    "    scheduler_hyper = dict(lr_lambda = lambda epoch: 0.96 ** epoch)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR\n",
    "    \n",
    "    # Model specific hypers\n",
    "    sample_rate = 4_000\n",
    "    bins = 64\n",
    "    channels = 256\n",
    "    dilation_depth = 9\n",
    "    blocks = 1\n",
    "    \n",
    "    # Seed everything\n",
    "    seed = 12\n",
    "    H.seed_torch(seed, deterministic=True)\n",
    "    \n",
    "    # Wandb\n",
    "    to_log = dict(\n",
    "        seed = seed,\n",
    "        mode = mode,\n",
    "        debug = debug,\n",
    "        device = device,\n",
    "        epochs=epochs,\n",
    "        batch_size = batch_size,\n",
    "        criterion = criterion,\n",
    "        optimizer = (optimizer, optimizer_hyper),\n",
    "        scheduler = (scheduler, scheduler_hyper),\n",
    "        model_hypers = {\"bins\":bins, \"channels\":channels, \n",
    "                        \"dilation_depth\":dilation_depth, \"blocks\":blocks},\n",
    "        dataset=\"https://github.com/Jakobovski/free-spoken-digit-dataset\",\n",
    "        architecture=\"WaveNet with g2p embeddings\",\n",
    "        notes=\"Audio synthesis with numbers ranging from 0-9. Only Local embeddings used.\"\n",
    "    )\n",
    "\n",
    "C = Config()\n",
    "if C.use_wandb:\n",
    "    wandb.login()\n",
    "    wandb.init(project=\"WaveNet\", config=C.to_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.) Preprocess audio\n",
    "1. Overview\n",
    "2. Downsample\n",
    "3. Maximum sound length and zero padding\n",
    "4. Apply preprocessing - downsample and cutoff\n",
    "5. Train and validation split\n",
    "6. EmbeddingNet - Local and global embeddings\n",
    "7. Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The dataset contains 3,000 audio files of spoken digits ranging from 0-9. The file format is `.wav` and the freqency or sampling rate is 8,000 Hz. The audio files are recorded by 6 different speakers: `george`, `jackson`, `lucas`, `nicolas`, `theo`, `yweweler` whose responible for exactly 500 audio files each (50 for each digit). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = C.audio_paths[0]\n",
    "waveform, sample_rate = torchaudio.load(sample_path)\n",
    "H.jupyter_play_audio(sample_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speakers (speaker distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(path):\n",
    "    string = os.path.basename(path)\n",
    "    return_string = \"\"\n",
    "    for c in string:\n",
    "        if c.isalpha(): return_string += c\n",
    "    return return_string\n",
    "\n",
    "# Unique speakers\n",
    "names = [ extract_name(path) for path in C.audio_paths ]\n",
    "unique_speakers = set(names)\n",
    "print(\"speakers: \\n\", unique_speakers, \"\\n\")\n",
    "\n",
    "# Distribution over number files recorded by each speaker\n",
    "files_recorded = {speaker:0 for speaker in unique_speakers}\n",
    "for name in names:\n",
    "    files_recorded[name] += 1\n",
    "print(\"Files recorded by count: \\n\", files_recorded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample\n",
    "A sampling rate of 8,000 Hz is relatively low (think the standard is 16,000 Hz and above). I'm going to downsmaple despite of this, because it makes the problem so much more tractable. After playing around with different sampling rates, I concluded that a reduction of 50% (4,000 Hz) seemed reasonable, comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_sampler = torchaudio.transforms.Resample(sample_rate, 4000)\n",
    "\n",
    "print(\"Freqency 8k (original)\")\n",
    "display(Audio(waveform, rate=8000))\n",
    "print(\"Freqency 4k (down sampled)\")\n",
    "Audio(down_sampler(waveform), rate=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum sound length and zero padding\n",
    "WaveNet dosn't handle different sized inputs well. Technically speaking, one could get away with just making every batch the same size, but this is more trouble than it's worth in my opinion.\n",
    "<br>\n",
    "This means that we need to specify a maximum length for the audio files and use this to truncate/zero-pad every sound longer/shorter than this. \n",
    "<br>\n",
    "An obvious maximum length would to the length of the longest audio file, but this is super wasteful. In the plot below, it's clear to see that the vast majority of the sounds are way shorter than that of the longest. There's no right solution here, but i think having 95\\% of the files remain un-truncated would be reasonble. This would mean the cutoff would be the 95\\% quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_lengths = []\n",
    "for path in C.audio_paths:\n",
    "    audio_lengths.append(torchaudio.load(path)[0].squeeze(0).shape[0])\n",
    "\n",
    "_, ax = plt.subplots(figsize=(15,6))\n",
    "ax.set_title(f\"Audio lengths - histogram\\n min: {min(audio_lengths)} | max: {max(audio_lengths)} | median: {np.median(audio_lengths):.0f}\")\n",
    "sns.histplot(audio_lengths, bins=50, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = int(np.ceil(np.quantile(audio_lengths, 0.95)))\n",
    "sorted_lengths = sorted(audio_lengths)\n",
    "threshold_index = np.searchsorted(sorted_lengths, threshold)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(15,6))\n",
    "ax.plot(sorted_lengths, label=\"Audio lengths - Sorted\")\n",
    "ax.set_title(f\"Sorted audio lengths\")\n",
    "ax.plot([threshold_index, threshold_index], [0, 18_000], '--', label=f\"Threshold: {threshold}\")\n",
    "ax.legend()\n",
    "\n",
    "threshold = threshold//2 # going to downsample before appling the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply preprocessing - downsample and cutoff \n",
    "I'm going to apply both downsample and length cutoff (truncate/zero padd) before training to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "\n",
    "# Going to put everything in a dataframe for convenience.\n",
    "df = pd.DataFrame( \n",
    "    {\"path\":[\"\"]*3000, \"speaker\": [\"\"]*3000, \"number\": [\"\"]*3000, \"pad_index\": [0]*3000} \n",
    ")\n",
    "\n",
    "for i, path in enumerate(tqdm(C.audio_paths)):\n",
    "    waveform, sample_rate = torchaudio.load(path)\n",
    "    new_waveform = down_sampler(waveform)\n",
    "    \n",
    "    if len(new_waveform) > threshold: # Truncate\n",
    "        pad_index = threshold\n",
    "        new_waveform = new_waveform[:, :threshold]\n",
    "    else: # Zero pad\n",
    "        pad_index = new_waveform.shape[1]\n",
    "        num_zero_pad = threshold - new_waveform.shape[1]\n",
    "        new_waveform = F.pad(new_waveform, pad=(0,num_zero_pad))\n",
    "    \n",
    "    assert new_waveform.shape[1] == threshold, \"Should not have happend\" # Just to be sure\n",
    "    \n",
    "    # Book keeping\n",
    "    new_path = f\"./data/{os.path.basename(path)}\"\n",
    "    number, speaker = os.path.basename(path).split(\"_\")[:-1]\n",
    "    df.iloc[i] = [new_path, speaker, str(number), pad_index]\n",
    "    torchaudio.save(new_path, src=new_waveform, sample_rate=4000)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validation split\n",
    "It's not immediately obvious how to do the split, but the follwing two approaches seem reasonable:\n",
    "<br>\n",
    "1. Use all audio files from a single speaker as validition set (would yeild 500 validation samples corrosponding to \\~16.7% of the data) and the remaining files as training data\n",
    "2. Do a random, but stratified, split over all speakers.\n",
    "<br>\n",
    "\n",
    "Both of these has obvious problems. Case 1. seems a bit unfair because the model has never heard that particular voice before. One could argue this would show generlazation capabilities, but I don't think this is necesarily true bevause we're dealing with sound synthesis. Case 2. may suffer from train-validition leakage, since it's reasonable to assume that digits spoken by the same speaker are very similiar. I do however think it's the best of the two, but it could be fun to test case 1. as well if time permits it.\n",
    "<br>\n",
    "Since I don't have that much data, the split is going to be 90\\% for training and 10\\% for validation. There's two choices for stratification \"label\": speaker and number. I decided (somewhat arbitrarily) to use the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validiation split\n",
    "df_train, df_valid = train_test_split(df, test_size=0.1, stratify=df[\"number\"])\n",
    "\n",
    "# For debug\n",
    "if C.debug:\n",
    "    df_train = df_train.sample(C.batch_size)\n",
    "    df_valid = df_valid.sample(C.batch_size//2)\n",
    "\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_valid.reset_index(drop=True, inplace=True)\n",
    "display(df_train)\n",
    "display(df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmbeddingNet - Local and global embeddings\n",
    "`EmbeddingNet` is responsible for making global and local embeddings for the `WaveNet` model.\n",
    "<br>\n",
    "The global embeddings are simply made from speaker IDs (integers) which gets one hot encoded and send throguh some 1x1 convolution to make the channels add up and provide some expresseive power. \n",
    "<br>\n",
    "The local embeddings are made from phoneme predictions provided by [g2p](https://github.com/Kyubyong/g2p). These are one hot encoded, send through a \"learned upscaling\" net, interpolated and zero padded to match the shape of the waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    \"\"\" Provide global and local embeddings for WaveNet \"\"\"\n",
    "\n",
    "    def __init__(self, channels, max_length, num_of_speakers=None):\n",
    "        \"\"\"\n",
    "        :param num_of_speakers:     Number of speakers used in the global conditioning\n",
    "        :param channels:            Number of channels used in WaveNet\n",
    "        :param device:              \"cuda\" or \"cpu\"\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_of_speakers = num_of_speakers\n",
    "        self.channels = channels\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Local conditioning\n",
    "        self.local_encoder = G2p()\n",
    "        self.local_cond_tokens = [\" \", \",\", \"!\", \"?\", \"'\", \".\", \"<PAD>\"] + self.local_encoder.phonemes[4:]\n",
    "        self.local_cond_dict = {token: i for i, token in enumerate(self.local_cond_tokens)}\n",
    "        self.num_of_tokens = len(self.local_cond_tokens)\n",
    "\n",
    "        self.local_up_sample_net = nn.Sequential(\n",
    "            nn.Conv1d(self.num_of_tokens, self.channels, 1, 1),  # Correct #channels and remove one hot sparsity\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(self.channels, self.channels, 4, 8), nn.ReLU(),  # up-sample 1.\n",
    "            nn.ConvTranspose1d(self.channels, self.channels, 4, 6), nn.ReLU(),  # up-sample 2.\n",
    "            nn.ConvTranspose1d(self.channels, self.channels, 4, 4), nn.ReLU(),  # up-sample 3.\n",
    "        )\n",
    "\n",
    "        # Global conditioning\n",
    "        if num_of_speakers is not None:\n",
    "            assert self.channels // 4 <= self.num_of_tokens, \"The gradual increase in channel size makes no sense when true\"\n",
    "            self.global_net = nn.Sequential(\n",
    "                nn.Conv1d(self.num_of_speakers, self.channels // 4, 1, 1), nn.ReLU(),  # channel increase 1.\n",
    "                nn.Conv1d(self.channels // 4, self.channels // 2, 1, 1), nn.ReLU(),  # channel increase 2.\n",
    "                nn.Conv1d(self.channels // 2, self.channels, 1, 1), nn.ReLU(),  # channel increase 3.\n",
    "            )\n",
    "            \n",
    "    def forward(self, transcripts, pad_indices, speaker=None):\n",
    "        \"\"\"\n",
    "        :param transcripts: What is being said in the waveform (Batch_size, samples)\n",
    "        :param pad_indices: The index at which the waveforms were zero padded from.\n",
    "        :param speaker:     Unique speaker ID used in the waveform recordings\n",
    "        :return:\n",
    "            local_cond         : Up-sampled and vectorized phonemes tokens (batch_size, channels, # up-scaled samples)\n",
    "            global_cond        : Vectorized speaker id (batch_size, channels, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        if (speaker is not None) and (self.num_of_speakers is None):\n",
    "            raise ValueError(\n",
    "                \"`speaker` not allowed when initialization occured without specifying `num_of_speakers`\"\n",
    "            )\n",
    "\n",
    "        # Global and local conditioning\n",
    "        local_cond = self.get_local_cond(transcripts, pad_indices)\n",
    "        global_cond = self.get_global_cond(speaker) if (speaker is not None) else None\n",
    "\n",
    "        return local_cond, global_cond\n",
    "\n",
    "\n",
    "    def get_global_cond(self, speaker):\n",
    "        one_hotted = F.one_hot(speaker, num_classes=self.num_of_speakers)\n",
    "        correct_format = one_hotted.to(self.device).unsqueeze(0).permute(1, 2, 0).float()\n",
    "        return self.global_net(correct_format)\n",
    "\n",
    "\n",
    "    def get_local_cond(self, transcripts, pad_indices):\n",
    "        # 1.) Phonemes tokens for each transcript\n",
    "        # 2.) Up sampling\n",
    "        # 3.) Interpolation according to individual zero padding indices\n",
    "        # 4.) Extra zero padding to match `self.max_length`\n",
    "        for_tensor = []\n",
    "        for i, transcript in enumerate(transcripts):\n",
    "            token_strings = self.local_encoder(transcript)\n",
    "            token_ints = [self.local_cond_dict[s] for s in token_strings]\n",
    "            tensor = torch.tensor([[75, 42, 60, 53]], dtype=torch.long, device=\"cuda\")\n",
    "            one_hotted = F.one_hot(tensor, num_classes=self.num_of_tokens).transpose(1, 2).float()\n",
    "            up_sampled = self.local_up_sample_net(one_hotted)\n",
    "            pad_index = up_sampled.shape[-1]*2 if (pad_indices[i] == -1) else pad_indices[i] # necessary for generation\n",
    "            interpolated = F.interpolate(up_sampled, size=pad_index)\n",
    "            padded = F.pad(interpolated, pad=(0, self.max_length - interpolated.shape[-1]))\n",
    "            for_tensor.append(padded)\n",
    "\n",
    "        return torch.cat(for_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloaders\n",
    "The `DigitDataset` is a pretty standard custom dataset class designed to be used in Pytorch's `DataLoader` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitDataset:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.mu_encoder = torchaudio.transforms.MuLawEncoding(quantization_channels=C.bins)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path, speaker, number, cutoff_index = self.data.iloc[i]\n",
    "        waveform = torchaudio.load(path)[0]\n",
    "        target_waveform = self.mu_encoder(waveform)\n",
    "        pred_waveform = F.one_hot(target_waveform.long(), num_classes=C.bins).transpose(1, 2).float()\n",
    "        \n",
    "        return pred_waveform[0], target_waveform[0], speaker, number, cutoff_index\n",
    "\n",
    "train_dataset = DigitDataset(df_train)\n",
    "valid_dataset = DigitDataset(df_valid)\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(train_dataset, batch_size=C.batch_size, shuffle=True)\n",
    "dl_valid = torch.utils.data.DataLoader(valid_dataset, batch_size=C.batch_size, shuffle=False)\n",
    "\n",
    "del train_dataset, valid_dataset # No use for these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "1. Setup EmbeddingNet and WaveNet\n",
    "2. Setup los, optimizer and learning rate scheduler\n",
    "3. Training loop with Weights and Biases\n",
    "4. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup EmbeddingNet and WaveNet\n",
    "The role of `EmbeddingNet` has already been explained and my implementation of WaveNet was introduced in first section.\n",
    "<br>\n",
    "The parameter count for both models is in the neighbourhood 6 milion parameters, which seems very reasonable â”€ perhaps even to modest, that is the models may not have a sufficient number of parameters to fulfil the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wavenet(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch of implementation of the original wavenet paper:\n",
    "    https://arxiv.org/pdf/1609.03499.pdf\n",
    "    Figure 3 in the paper summarizes the main ideas behind the model quite nicely\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_net, quantization_bins=64, channels=128, dilation_depth=9, blocks=1, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        :param quantization_bins:   Number of possible sound amplitudes (e.g. 256 would be a 8 bit repr.)\n",
    "        :param channels:            Number of channels used in all convolutions\n",
    "        :param dilation_depth:      Number of dilated causal convolutional layers within each stack.\n",
    "        :param blocks:              Number of blocks of stacked dilated causal convolutional layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.C = channels\n",
    "        self.bins = quantization_bins\n",
    "        self.dilations = [2 ** i for i in range(dilation_depth)] * blocks\n",
    "        self.receptive_field = sum(self.dilations)\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "        # Init model architecture\n",
    "        self.pre_process_conv = nn.Conv1d(in_channels=self.bins, out_channels=self.C, kernel_size=1)\n",
    "\n",
    "        self.causal_layers = nn.ModuleList()\n",
    "        for d in self.dilations:\n",
    "            self.causal_layers.append(ResidalLayer(channels=self.C, dilation=d))\n",
    "\n",
    "        self.post_process_conv1 = nn.Conv1d(self.C, self.C, kernel_size=1)\n",
    "        self.post_process_conv2 = nn.Conv1d(self.C, self.bins, kernel_size=1)\n",
    "\n",
    "        # Set everything to correct device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, waveform:torch.tensor, transcripts:list, pad_indices:list, speaker:int=None):\n",
    "        \"\"\"\n",
    "        :param waveform:    Mu- and one-hot-encoded waveform. (batch_size, quantization_bins, samples).\n",
    "        :param transcripts: What is being said in the waveform (Batch_size, samples)\n",
    "        :param pad_indices: The index at which the waveforms were zero padded from. (batch_size)\n",
    "        :param speaker:     Unique speaker ID used in the waveform recordings (batch_size)\n",
    "\n",
    "        :return:\n",
    "        Distribution over predictions for the next sample. (batch_size, quantization_bins, samples)\n",
    "        \"\"\"\n",
    "        # TODO: Check input shape and type\n",
    "\n",
    "        # Pre-processing\n",
    "        local_cond, global_cond = self.embedding_net(transcripts, pad_indices, speaker=None)\n",
    "        return self._forward(waveform, local_cond, global_cond)\n",
    "\n",
    "\n",
    "    def _forward(self, waveform, local_cond, global_cond ):\n",
    "        # Pre-processing\n",
    "        x = self.pre_process_conv(waveform)\n",
    "\n",
    "        # Dilated causal convolutions\n",
    "        skips, skip = [], None\n",
    "\n",
    "        for layer in self.causal_layers:\n",
    "            x, skip = layer(x, local_cond, global_cond)\n",
    "            skips.append(skip)\n",
    "\n",
    "        # Post processes (-softmax)\n",
    "        x = sum([s[:, :, -skip.size(2):] for s in skips]) # Sum all the skips values made\n",
    "        x = F.relu(x)\n",
    "        x = self.post_process_conv1(x)  # shape --> (batch_size, channels, samples)\n",
    "        x = F.relu(x)\n",
    "        x = self.post_process_conv2(x)  # shape --> (batch_size, quantization_bins, samples)\n",
    "        return x\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, transcript:str, seed:list = None, speaker:int = None):\n",
    "        \"\"\"\n",
    "        Warning: This is as slow as it gets and could definitely be improved.\n",
    "\n",
    "        :param transcript:  Text used for global embeddings\n",
    "        :param seed:        Seed to start of the generation (if None --> random seed)\n",
    "        :param speaker:     Speaker ID\n",
    "        :return:            List of generated samples\n",
    "        \"\"\"\n",
    "        local_emb, global_emb = self.embedding_net([transcript], [-1], speaker)\n",
    "\n",
    "        # Generate random seed if seed is None\n",
    "        temp = seed\n",
    "        if temp is None:\n",
    "            temp = np.random.randint(self.bins//2 - 1, self.bins//2 + 1, self.receptive_field + 1)\n",
    "\n",
    "        for n in tqdm(range(local_emb.shape[2] - self.receptive_field), leave=False):\n",
    "            x = torch.tensor(temp[-(self.receptive_field + 1):]).to(self.device)\n",
    "            x =  F.one_hot(x.unsqueeze(0).long(), num_classes=self.bins).transpose(1,2).float()\n",
    "\n",
    "            predictions = self._forward(x, local_emb[:, :, n: n+self.receptive_field], global_emb)\n",
    "            predictions = torch.softmax(predictions, dim=1)\n",
    "            sampled_index = torch.multinomial(predictions[0, :, 0], 1).squeeze()\n",
    "            temp.append(sampled_index.item())\n",
    "\n",
    "        return temp\n",
    "\n",
    "\n",
    "class ResidalLayer(nn.Module):\n",
    "    \"\"\" Wavenet residual layer (with a few modifications)\"\"\"\n",
    "\n",
    "    def __init__(self, channels:int, dilation:int, do_local_cond:bool=True, do_global_cond:bool=False):\n",
    "        \"\"\"\n",
    "        :param in_channels:     1D Convolution: Number of input channels\n",
    "        :param out_channels:    1D Convolution: Number of output channels\n",
    "        :param dilation:        1D Convolution: dilation\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dilation = dilation\n",
    "        self.do_local_cond = do_local_cond\n",
    "        self.do_global_cond = do_global_cond\n",
    "\n",
    "        # Gated activation. conv_f (filter) and conv_g (gate).\n",
    "        self.conv_f = nn.Conv1d(channels, channels, kernel_size=2, dilation=dilation)\n",
    "        self.conv_g = nn.Conv1d(channels, channels, kernel_size=2, dilation=dilation)\n",
    "\n",
    "        # Local conditioning\n",
    "        if do_local_cond is not None:\n",
    "            self.conv_filter_local = nn.Conv1d(channels, channels, kernel_size=1, dilation=dilation)\n",
    "            self.conv_gate_local = nn.Conv1d(channels, channels, kernel_size=1, dilation=dilation)\n",
    "\n",
    "        # Global conditioning\n",
    "        if do_global_cond is not None:\n",
    "            self.conv_filter_global = nn.Conv1d(channels, channels, kernel_size=1, dilation=dilation)\n",
    "            self.conv_gate_global = nn.Conv1d(channels, channels, kernel_size=1, dilation=dilation)\n",
    "\n",
    "        self.conv_1x1 = nn.Conv1d(channels, channels, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, waveform, local_cond, global_cond):\n",
    "        # General filter and gate preprocess\n",
    "        filter_in = self.conv_f(waveform)\n",
    "        gate_in = self.conv_g(waveform)\n",
    "\n",
    "        # Local conditioning preprocess\n",
    "        if self.do_local_cond:\n",
    "            local_cond = local_cond[:, :, -filter_in.shape[2]:]\n",
    "            filter_in += self.conv_filter_local(local_cond)\n",
    "            gate_in   += self.conv_gate_local(local_cond)\n",
    "\n",
    "        # Global conditioning preprocess\n",
    "        if self.do_global_cond:\n",
    "            filter_in += self.conv_filter_global(global_cond)\n",
    "            gate_in   += self.conv_gate_global(global_cond)\n",
    "\n",
    "        # Gated activation\n",
    "        filter_out = torch.tanh(filter_in)\n",
    "        gate_out = torch.sigmoid(filter_out)\n",
    "        filter_and_gate_combined = filter_out * gate_out\n",
    "\n",
    "        # Residual for next residual layer\n",
    "        skip = self.conv_1x1(filter_and_gate_combined)\n",
    "        residual = waveform[:, :, self.dilation:] + skip\n",
    "\n",
    "        return residual, skip\n",
    "\n",
    "\n",
    "# Init WaveNet\n",
    "embedding_net = EmbeddingNet(C.channels, threshold)\n",
    "wavenet = Wavenet(embedding_net, C.bins, C.channels, C.dilation_depth, C.blocks, C.device)\n",
    "print(\"wavenet + embedding_net parameter count: \", H.get_parameter_count(wavenet))\n",
    "\n",
    "# Sanity check (single forward pass)\n",
    "waveform, target, speaker, transcript, cutoff_index = next(iter(dl_train))\n",
    "print(wavenet( waveform.to(C.device), transcript, cutoff_index, speaker).shape)\n",
    "del waveform, target, speaker, transcript, cutoff_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup loss, optimizer and learning rate scheduler\n",
    "Cross entropy is a no brainer for loss because WaveNet outputs a probability distribution.\n",
    "<br> \n",
    "Adam was used as optimizer with a start learning rate of 1e-3 and Pytorch's `LambdaLR` was used as LR-scheduler.\n",
    "<br>\n",
    "The LR-scheduler was found with a small Jupyter notebook GUI I have written see [here](https://github.com/Jako-K/schedulerplotter). Picture below:\n",
    "<br>\n",
    "<p align=\"center\">\n",
    "    <img src=\"./readme_res/scheduler_plotter.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = C.optimizer(wavenet.parameters(), **C.optimizer_hyper)\n",
    "scheduler = C.scheduler(optimizer, **C.scheduler_hyper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "I've used a pretty standard Pytorch training loop with validation, logging, and best model saving. Something like: \n",
    "```python\n",
    "model, optimizer, schedular = init_model()\n",
    "train_dl, valid_dl = init_data()\n",
    "\n",
    "for epoch in epochs:\n",
    "    do_training_stuff(model, optimizer, train_dl)\n",
    "    do_validation_stuff(model, valid_dl)\n",
    "    do_update_and_logging_stuff(schedular, model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "#                                             Setup                                               #\n",
    "###################################################################################################\n",
    "\n",
    "if C.use_wandb and not C.wandb_watch_activated: \n",
    "    C.wandb_watch_activated = True\n",
    "    wandb.watch(wavenet, C.criterion, log=\"all\", log_freq=10)\n",
    "    \n",
    "stats = pd.DataFrame(np.zeros((C.epochs,3)), columns=[\"train_loss\", \"valid_loss\", \"learning_rate\"])\n",
    "best_model_name = \"0_EPOCH.pth\"\n",
    "print(H.get_gpu_memory_info())\n",
    "\n",
    "###################################################################################################\n",
    "#                                          Training                                               #\n",
    "###################################################################################################\n",
    "\n",
    "for epoch in tqdm(range(C.epochs)):\n",
    "    train_losses, valid_losses = np.zeros(len(dl_train)), np.zeros(len(dl_valid))\n",
    "\n",
    "    wavenet.train()\n",
    "    for i, batch in enumerate(tqdm(dl_train, leave=False)):\n",
    "        waveform, target, speaker, transcript, cutoff_index = batch\n",
    "        waveform, target = waveform.to(C.device), target.to(C.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        preds = wavenet(waveform, transcript, cutoff_index, speaker)\n",
    "        \n",
    "        # Calculates loss. The whole indexation show is just to align predictions with the targets.\n",
    "        loss = C.criterion(preds[:, :, :-1], target[:, -preds.size(2)+1:])\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Batch update and logging\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_losses[i] = loss.detach().cpu().item()\n",
    "        \n",
    "    wavenet.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dl_valid, leave=False)):\n",
    "            waveform, target, speaker, transcript, cutoff_index = batch\n",
    "            waveform, target = waveform.to(C.device), target.to(C.device)\n",
    "            \n",
    "            # Forward pass and loss\n",
    "            preds = wavenet(waveform, transcript, cutoff_index, speaker)\n",
    "            loss = C.criterion(preds[:, :, :-1], target[:, -preds.size(2)+1:])\n",
    "            \n",
    "            # Batch update and logging\n",
    "            valid_losses[i] = loss.detach().cpu().item()\n",
    "    \n",
    "    # Epoch update and logging\n",
    "    train_mean, valid_mean = train_losses.mean(), valid_losses.mean()\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    stats.iloc[epoch] = [train_mean, valid_mean, lr]\n",
    "    scheduler.step()\n",
    "    C.epochs_trained += 1\n",
    "    \n",
    "    if C.use_wandb:\n",
    "        wandb.log({\"train_loss\": train_mean, \"valid_loss\": valid_mean, \"lr\":lr})\n",
    "    \n",
    "    # Save model if it's better than the previous best\n",
    "    if (epoch > 0) and (stats[\"valid_loss\"][epoch] < stats[\"valid_loss\"][epoch-1]):\n",
    "        extra_info = {\"valid_loss\":valid_mean, \"epochs_trained\":C.epochs_trained}\n",
    "        best_model_name = T.get_model_save_name(extra_info, \"model.pth\", include_time=True)\n",
    "        torch.save(wavenet.state_dict(), best_model_name)\n",
    "\n",
    "###################################################################################################\n",
    "#                                          Finish up                                              #\n",
    "###################################################################################################\n",
    "        \n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, figsize=(20,10))\n",
    "stats.drop(columns=[\"learning_rate\"]).plot(ax=ax1, style=\".-\")\n",
    "ax2.ticklabel_format(style=\"scientific\", axis='y', scilimits=(0,0))\n",
    "stats[\"learning_rate\"].plot(ax=ax2, style=\".-\")\n",
    "\n",
    "# Save model\n",
    "if C.use_wandb:\n",
    "    shutil.copy(best_model_name, os.path.join(wandb.run.dir, best_model_name))\n",
    "    wandb.save(best_model_name, policy=\"now\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evalutation - sound synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best model\n",
    "Use the model with the lowest validation loss for sound synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(best_model_name)\n",
    "wavenet.load_state_dict(checkpoint)\n",
    "wavenet.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for sound synthesis\n",
    "mu_encoder = torchaudio.transforms.MuLawEncoding(quantization_channels=C.bins)\n",
    "mu_decoder = torchaudio.transforms.MuLawDecoding(quantization_channels=C.bins)\n",
    "down_sampler = torchaudio.transforms.Resample(8000, 4000)\n",
    "\n",
    "# Combine everything related to sound synthesis in one function\n",
    "def generate_all(transcript, model, save_name, seed): \n",
    "    if seed is not None:\n",
    "        seed = down_sampler(seed)\n",
    "        seed = mu_encoder(seed[:, :wavenet.receptive_field+1])[0].tolist()\n",
    "        \n",
    "    # Generate and decode sound\n",
    "    gen = wavenet.generate(transcript, seed)\n",
    "    mu_decoded_gen = mu_decoder(torch.tensor(gen))\n",
    "    gen_export = mu_decoded_gen.view(1,-1).float()\n",
    "    \n",
    "    # Save sound   \n",
    "    torchaudio.save(f'{save_name}_generated.wav', gen_export, sample_rate=C.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual audio generation\n",
    "Generate digit sounds (0-9) from seeds found in `./digits_for_generation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sound generation and seeds\n",
    "for i in range(10):\n",
    "    seed = torchaudio.load(f\"./data/{i}_yweweler_0.wav\")[0]\n",
    "    generate_all(f\"{i}\", wavenet, save_name=f\"final_{i}\", seed=seed)\n",
    "\n",
    "# Combine all 10 sounds to a single .wav\n",
    "generated_sounds_combined = torch.cat( \n",
    "    [torchaudio.load(f\"final_{i}_generated.wav\")[0][0] for i in range(10)] \n",
    ").unsqueeze(0)\n",
    "torchaudio.save(f'combined.wav', generated_sounds_combined, sample_rate=4000)\n",
    "\n",
    "# Make the combined sound playable and plot it\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "display(Audio('combined.wav', rate=4000))\n",
    "plt.plot(generated_sounds_combined[0])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
